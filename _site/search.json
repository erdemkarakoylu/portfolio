[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/causailty_in_observation_data/index.html",
    "href": "posts/causailty_in_observation_data/index.html",
    "title": "Evaluating causality in observational data",
    "section": "",
    "text": "This post illustrates some ways of inferring causality in observational data. Establishing causality of a treatment or similar intervention is more easily done under conditions where all possible factors that may affect the outcome can be controlled for; what is referred to as Randomized Control Trials (RCT.)\nEstablishing causal link in observational studies can be quite challenging. This is because there can be many potential confounders, not all of which might be identified. By confounder I mean “a variable whose presence affects the variables being studied so that the results do not reflect the actual relationship” (Pourhoseingholi et al. 2012.)\nThis project illustrates the use of matching and propensity score to establish a causal link beween treatment and outcome in an observational study by Lalonde (1986); a pdf of the paper is available here. The goal of the study was to evaluate the impact of an employment and training program. The data I use here is a subset of the data generated by this study; it includes 614 observations in total with 10 variables. This data is included in Matching; one of the libraries I use for this project. The data is loaded as follows.\n\n\nCode\nlibrary(MatchIt)\ndata(lalonde)\n\n\nThe loaded data includes a number of covariates, an outcome variable and a treatment flag indicating whether the subject was part of the control or the treatment group. These variables are named and summarized in the table below.\n\n\n\nVariable\nSummary\n\n\n\n\nage1\nin floating point years\n\n\nrace1\nOne of Black, Hispanic, White\n\n\neduc1\nyears of schooling\n\n\nmarried1\nBoolean for marital status\n\n\nnodegree1\nBoolean for high school diploma\n\n\nre741\nreal earnings in 1974\n\n\nre751\nreal earnings in 1975\n\n\nre782\nreal earnings in 1978\n\n\ntreat3\nBoolean for treatment status\n\n\n\nFor convenience, I one-hot encode the race variable, and cast it in its new format along with the rest of the data in a new table that follows. Note that in the present subset of this data, only black and white subjects were available. I therefore do not include hispanic as a covariate in the analysis that follows. For convenience, I also change the outcome variable, \\(re78\\) to the more meaningful name \\(outcome\\).\n\n\nCode\nhispan&lt;-as.numeric(lalonde$race=='hispan')\nblack&lt;-as.numeric(lalonde$race=='black')\nwhite&lt;-as.numeric(lalonde$race=='white')\nage&lt;-lalonde$age\neduc&lt;-lalonde$educ\nmarried&lt;-lalonde$married\nnodegree&lt;-lalonde$nodegree\nre74&lt;-lalonde$re74\nre75&lt;-lalonde$re75\ntreat&lt;-lalonde$treat\noutcome&lt;-lalonde$re78\nmydata&lt;-cbind(age, educ, married, nodegree, black, white, hispan, \n              re74, re75, treat, outcome)\nmydata&lt;-data.frame(mydata)\n\n\nAll covariates are expected to be confounders. Thus it is important to evaluate whether the data is balanced between treatment and control groups; i.e. whether the covariates are similarly distributed between the two groups. If they are then the analysis can proceed. Otherwise, the data needs to be balanced. One way to balance data is to use matching; another will use something called propensity score. Next, I will illustrate both appraoches."
  },
  {
    "objectID": "posts/causailty_in_observation_data/index.html#preamble-causality-in-observational-data",
    "href": "posts/causailty_in_observation_data/index.html#preamble-causality-in-observational-data",
    "title": "Evaluating causality in observational data",
    "section": "",
    "text": "This post illustrates some ways of inferring causality in observational data. Establishing causality of a treatment or similar intervention is more easily done under conditions where all possible factors that may affect the outcome can be controlled for; what is referred to as Randomized Control Trials (RCT.)\nEstablishing causal link in observational studies can be quite challenging. This is because there can be many potential confounders, not all of which might be identified. By confounder I mean “a variable whose presence affects the variables being studied so that the results do not reflect the actual relationship” (Pourhoseingholi et al. 2012.)\nThis project illustrates the use of matching and propensity score to establish a causal link beween treatment and outcome in an observational study by Lalonde (1986); a pdf of the paper is available here. The goal of the study was to evaluate the impact of an employment and training program. The data I use here is a subset of the data generated by this study; it includes 614 observations in total with 10 variables. This data is included in Matching; one of the libraries I use for this project. The data is loaded as follows.\n\n\nCode\nlibrary(MatchIt)\ndata(lalonde)\n\n\nThe loaded data includes a number of covariates, an outcome variable and a treatment flag indicating whether the subject was part of the control or the treatment group. These variables are named and summarized in the table below.\n\n\n\nVariable\nSummary\n\n\n\n\nage1\nin floating point years\n\n\nrace1\nOne of Black, Hispanic, White\n\n\neduc1\nyears of schooling\n\n\nmarried1\nBoolean for marital status\n\n\nnodegree1\nBoolean for high school diploma\n\n\nre741\nreal earnings in 1974\n\n\nre751\nreal earnings in 1975\n\n\nre782\nreal earnings in 1978\n\n\ntreat3\nBoolean for treatment status\n\n\n\nFor convenience, I one-hot encode the race variable, and cast it in its new format along with the rest of the data in a new table that follows. Note that in the present subset of this data, only black and white subjects were available. I therefore do not include hispanic as a covariate in the analysis that follows. For convenience, I also change the outcome variable, \\(re78\\) to the more meaningful name \\(outcome\\).\n\n\nCode\nhispan&lt;-as.numeric(lalonde$race=='hispan')\nblack&lt;-as.numeric(lalonde$race=='black')\nwhite&lt;-as.numeric(lalonde$race=='white')\nage&lt;-lalonde$age\neduc&lt;-lalonde$educ\nmarried&lt;-lalonde$married\nnodegree&lt;-lalonde$nodegree\nre74&lt;-lalonde$re74\nre75&lt;-lalonde$re75\ntreat&lt;-lalonde$treat\noutcome&lt;-lalonde$re78\nmydata&lt;-cbind(age, educ, married, nodegree, black, white, hispan, \n              re74, re75, treat, outcome)\nmydata&lt;-data.frame(mydata)\n\n\nAll covariates are expected to be confounders. Thus it is important to evaluate whether the data is balanced between treatment and control groups; i.e. whether the covariates are similarly distributed between the two groups. If they are then the analysis can proceed. Otherwise, the data needs to be balanced. One way to balance data is to use matching; another will use something called propensity score. Next, I will illustrate both appraoches."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Some of my work…",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\nEvaluating causality in observational data\n\n\n\n\n\n\n\ncausality\n\n\npropensity score\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2022\n\n\nErdem Karakoylu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my Blog-o-folio!",
    "section": "",
    "text": "This site is meant to help those - including me! - that need a different take on a technical topic of interest. The site is also meant to educate and entertain with entries Statistics, Data Science, Modeling in a variety of fields. If you dig deep enough, you’ll even fine some entries on Marine Ecology Modeling; for such was my journey. Enjoy, and by all means do leave a comment. Regardless of its polarity I find feedback a useful improvement tool.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\nEvaluating causality in observational data\n\n\n\n\n\n\n\ncausality\n\n\npropensity score\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2022\n\n\nErdem Karakoylu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Erdem Karakoylu",
    "section": "",
    "text": "I am currently a Data Scientist at Research Innovations Inc (RII), where my various projects span a number of topics including sentiment analysis, directed sentiment analysis, fake news detection, uncertainty estimation in deep learning, graph analytics, and graph neural networks. For the past 10 years, my goto programming language for these and other analytics and modeling tasks has been Python, though I sometimes fire up R studio to code up some quick statistical analysis in R.\nBefore RII I was a scientific programmer at the NASA Ocean Biology Processing Group, where I cut my teeth coding up Bayesian models to estimate the prediction uncertainty for a variety of satellite processing algorithms. These algorithms are the backbone of product generation for data generated by legacy and active satellite missions, as well as the PACE (Plankton Aerosol Cloud Ecosystem) mission scheduled to begin in 2024.\nPrior to my analytics/data science career, I trained as an oceanographer. I graduated with a PhD from Scripps Institution of Oceanography, where I primarily focused on the ecology of tiny organisms that nevertheless are tremendously important in ocean ecological processes and have ramifications for both our supply of seafood (protein!) as well as climate change."
  },
  {
    "objectID": "posts/causailty_in_observation_data/index.html#section",
    "href": "posts/causailty_in_observation_data/index.html#section",
    "title": "Evaluating causality in observational data",
    "section": "",
    "text": "To match or not to match\nA commonly used metric to figure out whether balancing the data is required is Standardized Mean Difference (\\(SMD\\)), defined as the difference between group means divided by the pooled standard deviation, like so:\n\\[\nSMD = \\frac{\\bar{X}_{treatment}-\\bar{X}_{control}}\n{\\sqrt{\\frac{s^2_{treatment}+s^2_{control}}{2}}}\n\\] An easy way to examine covariates is to cast them into what is know as a Table 1, after a common pattern in the biomedical research litterature to feature patient attributes in the first table of published papers. The R library tableone is commonly used for this purpose, with the added benefit that the SMD is given out of the box as shown below. Here the data is stratified by treatment group and only the covariates are tabulated.\n\n\nCode\nlibrary(tableone)\n\n# Make a vector of the variable names to be used\nxvars &lt;- c(\"black\", \"white\", \"age\",\"educ\",\"married\",\"nodegree\",\n           \"re74\", \"re75\")\n# load to a table 1\ntable1 &lt;- CreateTableOne(vars=xvars, strata=\"treat\", data=mydata)\n\n\nWarning in ModuleReturnVarsExist(vars, data): The data frame does not have:\nblack white nodegree Dropped\n\n\nCode\n# show table, in particular display SMDs corresponding to each covariate. \nprint(table1, smd=TRUE)\n\n\n                     Stratified by treat\n                      0                 1                 p      test SMD   \n  n                       260               185                             \n  age (mean (SD))       25.05 (7.06)      25.82 (7.16)     0.265       0.107\n  educ (mean (SD))      10.09 (1.61)      10.35 (2.01)     0.135       0.141\n  married (mean (SD))    0.15 (0.36)       0.19 (0.39)     0.327       0.094\n  re74 (mean (SD))    2107.03 (5687.91) 2095.57 (4886.62)  0.982       0.002\n  re75 (mean (SD))    1266.91 (3102.98) 1532.06 (3219.25)  0.382       0.084\n\n\nAn alternative would be to conduct two-tailed t-tests to assess difference between group means for each covariate, and evaluate their corresponding p-value. This is however not without drawbacks; most importantly the resulting p-value will depend on the sample size."
  },
  {
    "objectID": "posts/causailty_in_observation_data/index.html#matching",
    "href": "posts/causailty_in_observation_data/index.html#matching",
    "title": "Evaluating causality in observational data",
    "section": "Matching",
    "text": "Matching\n\nTo match or not to match?\nA first step is whether the data on hand is appropriate for causal inferrence, in particular, whether it should be balanced. A commonly used metric to figure out whether balancing the data is required is Standardized Mean Difference (\\(SMD\\)), defined as the difference between group means divided by the pooled standard deviation, like so:\n\\[\nSMD = \\frac{\\bar{X}_{treatment}-\\bar{X}_{control}}\n{\\sqrt{\\frac{s^2_{treatment}+s^2_{control}}{2}}}\n\\] An easy way to examine covariates is to cast them into what is know as a Table 1, after a common pattern in the biomedical research litterature to feature patient attributes in the first table of published papers. The R library tableone is commonly used for this purpose, with the added benefit that the SMD is given out of the box as shown below. Here the data is stratified by treatment group and only the covariates are tabulated.\n\n\nCode\nlibrary(tableone)\n\n# Make a vector of the variable names to be used\nxvars &lt;-c(\"hispan\", \"black\", \"white\", \"age\", \"educ\", \"married\", \"nodegree\", \n              \"re74\", \"re75\")\n# load to a table 1\ntable1 &lt;- CreateTableOne(vars=xvars, strata=\"treat\", data=mydata, test=FALSE)\n# show table, in particular display SMDs corresponding to each covariate. \nprint(table1, smd=TRUE)\n\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        429               185                 \n  hispan (mean (SD))      0.14 (0.35)       0.06 (0.24)     0.277\n  black (mean (SD))       0.20 (0.40)       0.84 (0.36)     1.668\n  white (mean (SD))       0.66 (0.48)       0.10 (0.30)     1.406\n  age (mean (SD))        28.03 (10.79)     25.82 (7.16)     0.242\n  educ (mean (SD))       10.24 (2.86)      10.35 (2.01)     0.045\n  married (mean (SD))     0.51 (0.50)       0.19 (0.39)     0.719\n  nodegree (mean (SD))    0.60 (0.49)       0.71 (0.46)     0.235\n  re74 (mean (SD))     5619.24 (6788.75) 2095.57 (4886.62)  0.596\n  re75 (mean (SD))     2466.48 (3292.00) 1532.06 (3219.25)  0.287\n\n\nNote that an alternative would be to conduct two-tailed t-tests to assess difference between group (treated and control) means for each covariate, and evaluate their corresponding p-value. This is however not without drawbacks; most importantly the resulting p-value will depend on the sample size. I therefore use \\(SMD\\) in this post.\nBy convention, an \\(SMD\\) greater than 0.1 suggest an imbalance with respect to the corresponding covariate. Here \\(SMD&gt;0.1\\) for all covariates except education. Treated subjects need each to be match via greedy matching to as close as possible a control subject. Matching between subjects is done on the basis of a distance metric indicating how separated they are in the covariate space. The specific metric used in this case is the Mahalanobis distance, which is a kind of standardized difference, computed as follows: \\[d = \\sqrt{(X_i-X_j)^T C^{-1} (X_i-X_j) }\\] where \\(X\\) is a covariate, \\(i\\) and \\(j\\) are treated and control subjects, and \\(C\\) is the covariance matrix\n\n\nCode\nlibrary(Matching)\n# Below M=1 refers to pairwise matching. Even so if \"ties\" is left as TRUE (default)\n# multiple subjects within the tolerance threshold will all be matched. \n# In this case, e.g. not setting ties=TRUE yields 207 pairs, even though there are only # 185 treated subjects.\ngreedymatch&lt;-Match(Tr=treat, M=1, X=mydata[xvars], ties=FALSE) \nmatched&lt;-mydata[unlist(greedymatch[c(\"index.treated\", \"index.control\")]), ]\n\n\nI create another Table 1 with the matched data check the SMDs.\n\n\nCode\nmatchedtab1&lt;-CreateTableOne(vars=xvars, strata=\"treat\", data=matched, test=FALSE)\nprint(matchedtab1, smd=TRUE)\n\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        185               185                 \n  hispan (mean (SD))      0.06 (0.24)       0.06 (0.24)    &lt;0.001\n  black (mean (SD))       0.84 (0.37)       0.84 (0.36)     0.015\n  white (mean (SD))       0.10 (0.30)       0.10 (0.30)     0.018\n  age (mean (SD))        25.36 (8.29)      25.82 (7.16)     0.059\n  educ (mean (SD))       10.45 (1.96)      10.35 (2.01)     0.054\n  married (mean (SD))     0.19 (0.39)       0.19 (0.39)    &lt;0.001\n  nodegree (mean (SD))    0.71 (0.46)       0.71 (0.46)    &lt;0.001\n  re74 (mean (SD))     2159.92 (4240.18) 2095.57 (4886.62)  0.014\n  re75 (mean (SD))     1119.08 (2442.29) 1532.06 (3219.25)  0.145\n\n\nGreedy pairwise matching yields, as expected, a reduced data set with 185 subjects in each group. This time all but the variable \\(re75\\) have corresponding \\(SMD&lt;0.1\\). This is not entirely satisfactory and I will attempt to balance the data set using propensity scores next. Before doing so, however, I do a quick outcome analysis and compare the means of the two groups.\n\n\nCode\n# Outcome Analysis:\n# Carry a paired t-test on the matched data to get a causal risk difference\ntreated_outcome&lt;-matched$outcome[matched$treat==1]\ncontrol_outcome&lt;-matched$outcome[matched$treat==0]\nround(mean(treated_outcome) - mean(control_outcome), digits=2)\n\n\n[1] 173.05\n\n\nCode\n#pairwise difference\ndiff_outcome = treated_outcome - control_outcome\n\n\n#paired t-test\nt.test(diff_outcome)\n\n\n\n    One Sample t-test\n\ndata:  diff_outcome\nt = 0.25262, df = 184, p-value = 0.8008\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -1178.404  1524.494\nsample estimates:\nmean of x \n 173.0453 \n\n\nA p-value of .92 suggests the difference is not significant."
  },
  {
    "objectID": "posts/causailty_in_observation_data/index.html#propensity-scores",
    "href": "posts/causailty_in_observation_data/index.html#propensity-scores",
    "title": "Evaluating causality in observational data",
    "section": "Propensity scores",
    "text": "Propensity scores"
  }
]