[
  {
    "objectID": "posts/causailty_in_observation_data/index.html",
    "href": "posts/causailty_in_observation_data/index.html",
    "title": "Evaluating causality in observational data",
    "section": "",
    "text": "This post illustrates some ways of inferring causality in observational data. Establishing causality of a treatment or similar intervention is more easily done under laboratory conditions where all possible factors that may be a cause of the outcome can be controlled for, also known as is referred to as Randomized Control Trials (RCT.)\nEstablishing causal link in observational studies can be quite challenging. This is because there can be many potential confounders, not all of which might be identified. By confounder I mean “a variable whose presence affects the variables being studied so that the results do not reflect the actual relationship” (Pourhoseingholi et al. 2012.)\nThis project illustrates the use of matching and propensity score to establish a causal link beween treatment and outcome in an observational study by Lalonde (1986); a pdf of the paper is available here. The goal of the study was to evaluate the impact of an employment and training program. The data I use here is a subset of the data generated by this study; it includes 614 observations in total with 10 variables. This data is included in Matching; one of the libraries I use for this project. The data is loaded as follows.\n\n\nCode\nlibrary(MatchIt)\ndata(lalonde)\n\n\nThe loaded data includes a number of covariates, an outcome variable and a treatment flag indicating whether the subject was part of the control or the treatment group. These variables are named and summarized in the table below.\n\n\n\nVariable\nSummary\n\n\n\n\nage1\nin floating point years\n\n\nrace1\nOne of Black, Hispanic, White\n\n\neduc1\nyears of schooling\n\n\nmarried1\nBoolean for marital status\n\n\nnodegree1\nBoolean for high school diploma\n\n\nre741\nreal earnings in 1974\n\n\nre751\nreal earnings in 1975\n\n\nre782\nreal earnings in 1978\n\n\ntreat3\nBoolean for treatment status\n\n\n\nFor convenience, I one-hot encode the race variable, and cast it in its new format along with the rest of the data in a new table that follows. Note that in the present subset of this data, only black and white subjects were available. I therefore do not include hispanic as a covariate in the analysis that follows. For convenience, I also change the outcome variable, \\(re78\\) to the more meaningful name \\(outcome\\).\n\n\nCode\nhispan&lt;-as.numeric(lalonde$race=='hispan')\nblack&lt;-as.numeric(lalonde$race=='black')\nwhite&lt;-as.numeric(lalonde$race=='white')\nage&lt;-lalonde$age\neduc&lt;-lalonde$educ\nmarried&lt;-lalonde$married\nnodegree&lt;-lalonde$nodegree\nre74&lt;-lalonde$re74\nre75&lt;-lalonde$re75\ntreat&lt;-lalonde$treat\noutcome&lt;-lalonde$re78\nmydata&lt;-cbind(age, educ, married, nodegree, black, white, hispan, \n              re74, re75, treat, outcome)\nmydata&lt;-data.frame(mydata)\n\n\nAll covariates are expected to be confounders. Thus it is important to evaluate whether the data is balanced between treatment and control groups; i.e. whether the covariates are similarly distributed between the two groups. If they are then the analysis can proceed. Otherwise, the data needs to be balanced. One way to balance data is to use matching; another will use something called propensity score. Next, I will illustrate both appraoches."
  },
  {
    "objectID": "posts/causailty_in_observation_data/index.html#preamble-causality-in-observational-data",
    "href": "posts/causailty_in_observation_data/index.html#preamble-causality-in-observational-data",
    "title": "Evaluating causality in observational data",
    "section": "",
    "text": "This post illustrates some ways of inferring causality in observational data. Establishing causality of a treatment or similar intervention is more easily done under laboratory conditions where all possible factors that may be a cause of the outcome can be controlled for, also known as is referred to as Randomized Control Trials (RCT.)\nEstablishing causal link in observational studies can be quite challenging. This is because there can be many potential confounders, not all of which might be identified. By confounder I mean “a variable whose presence affects the variables being studied so that the results do not reflect the actual relationship” (Pourhoseingholi et al. 2012.)\nThis project illustrates the use of matching and propensity score to establish a causal link beween treatment and outcome in an observational study by Lalonde (1986); a pdf of the paper is available here. The goal of the study was to evaluate the impact of an employment and training program. The data I use here is a subset of the data generated by this study; it includes 614 observations in total with 10 variables. This data is included in Matching; one of the libraries I use for this project. The data is loaded as follows.\n\n\nCode\nlibrary(MatchIt)\ndata(lalonde)\n\n\nThe loaded data includes a number of covariates, an outcome variable and a treatment flag indicating whether the subject was part of the control or the treatment group. These variables are named and summarized in the table below.\n\n\n\nVariable\nSummary\n\n\n\n\nage1\nin floating point years\n\n\nrace1\nOne of Black, Hispanic, White\n\n\neduc1\nyears of schooling\n\n\nmarried1\nBoolean for marital status\n\n\nnodegree1\nBoolean for high school diploma\n\n\nre741\nreal earnings in 1974\n\n\nre751\nreal earnings in 1975\n\n\nre782\nreal earnings in 1978\n\n\ntreat3\nBoolean for treatment status\n\n\n\nFor convenience, I one-hot encode the race variable, and cast it in its new format along with the rest of the data in a new table that follows. Note that in the present subset of this data, only black and white subjects were available. I therefore do not include hispanic as a covariate in the analysis that follows. For convenience, I also change the outcome variable, \\(re78\\) to the more meaningful name \\(outcome\\).\n\n\nCode\nhispan&lt;-as.numeric(lalonde$race=='hispan')\nblack&lt;-as.numeric(lalonde$race=='black')\nwhite&lt;-as.numeric(lalonde$race=='white')\nage&lt;-lalonde$age\neduc&lt;-lalonde$educ\nmarried&lt;-lalonde$married\nnodegree&lt;-lalonde$nodegree\nre74&lt;-lalonde$re74\nre75&lt;-lalonde$re75\ntreat&lt;-lalonde$treat\noutcome&lt;-lalonde$re78\nmydata&lt;-cbind(age, educ, married, nodegree, black, white, hispan, \n              re74, re75, treat, outcome)\nmydata&lt;-data.frame(mydata)\n\n\nAll covariates are expected to be confounders. Thus it is important to evaluate whether the data is balanced between treatment and control groups; i.e. whether the covariates are similarly distributed between the two groups. If they are then the analysis can proceed. Otherwise, the data needs to be balanced. One way to balance data is to use matching; another will use something called propensity score. Next, I will illustrate both appraoches."
  },
  {
    "objectID": "posts/causailty_in_observation_data/index.html#matching",
    "href": "posts/causailty_in_observation_data/index.html#matching",
    "title": "Evaluating causality in observational data",
    "section": "Matching",
    "text": "Matching\n\nTo match or not to match?\nA first step is whether the data on hand is appropriate for causal inferrence, in particular, whether it should be balanced. A commonly used metric to figure out whether balancing the data is required is Standardized Mean Difference (\\(SMD\\)), defined as the difference between group means divided by the pooled standard deviation, like so:\n\\[\nSMD = \\frac{\\bar{X}_{treatment}-\\bar{X}_{control}}\n{\\sqrt{\\frac{s^2_{treatment}+s^2_{control}}{2}}}\n\\] An easy way to examine covariates is to cast them into what is know as a Table 1, after a common pattern in the biomedical research litterature to feature patient attributes in the first table of published papers. The R library tableone is commonly used for this purpose, with the added benefit that the SMD is given out of the box as shown below. Here the data is stratified by treatment group and only the covariates are tabulated.\n\n\nCode\nlibrary(tableone)\n\n# Make a vector of the variable names to be used\nxvars &lt;-c(\"hispan\", \"black\", \"white\", \"age\", \"educ\", \"married\", \"nodegree\", \n              \"re74\", \"re75\")\n# load to a table 1\ntable1 &lt;- CreateTableOne(vars=xvars, strata=\"treat\", data=mydata, test=FALSE)\n# show table, in particular display SMDs corresponding to each covariate. \nprint(table1, smd=TRUE)\n\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        429               185                 \n  hispan (mean (SD))      0.14 (0.35)       0.06 (0.24)     0.277\n  black (mean (SD))       0.20 (0.40)       0.84 (0.36)     1.668\n  white (mean (SD))       0.66 (0.48)       0.10 (0.30)     1.406\n  age (mean (SD))        28.03 (10.79)     25.82 (7.16)     0.242\n  educ (mean (SD))       10.24 (2.86)      10.35 (2.01)     0.045\n  married (mean (SD))     0.51 (0.50)       0.19 (0.39)     0.719\n  nodegree (mean (SD))    0.60 (0.49)       0.71 (0.46)     0.235\n  re74 (mean (SD))     5619.24 (6788.75) 2095.57 (4886.62)  0.596\n  re75 (mean (SD))     2466.48 (3292.00) 1532.06 (3219.25)  0.287\n\n\nNote that an alternative would be to conduct two-tailed t-tests to assess difference between group (treated and control) means for each covariate, and evaluate their corresponding p-value. This is however not without drawbacks; most importantly the resulting p-value will depend on the sample size. I therefore use \\(SMD\\) in this post.\nBy convention, an \\(SMD\\) greater than 0.1 suggest an imbalance with respect to the corresponding covariate. Here \\(SMD&gt;0.1\\) for all covariates except education. Treated subjects need each to be match via greedy matching to as close as possible a control subject. Matching between subjects is done on the basis of a distance metric indicating how separated they are in the covariate space. The specific metric used in this case is the Mahalanobis distance, which is a kind of standardized difference, computed as follows: \\[d = \\sqrt{(X_i-X_j)^T C^{-1} (X_i-X_j) }\\] where \\(X\\) is a covariate, \\(i\\) and \\(j\\) are treated and control subjects, and \\(C\\) is the covariance matrix\n\n\nCode\nlibrary(Matching)\n# Below M=1 refers to pairwise matching. Even so if \"ties\" is left as TRUE (default)\n# multiple subjects within the tolerance threshold will all be matched. \n# In this case, e.g. not setting ties=TRUE yields 207 pairs, even though there are only # 185 treated subjects.\ngreedymatch&lt;-Match(Tr=treat, M=1, X=mydata[xvars], ties=FALSE) \ngreedymatched&lt;-mydata[unlist(greedymatch[c(\"index.treated\", \"index.control\")]), ]\n\n\nI create another Table 1 with the matched data check the SMDs.\n\n\nCode\nmatchedtab1&lt;-CreateTableOne(vars=xvars, strata=\"treat\", data=greedymatched, test=FALSE)\nprint(matchedtab1, smd=TRUE)\n\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        185               185                 \n  hispan (mean (SD))      0.06 (0.24)       0.06 (0.24)    &lt;0.001\n  black (mean (SD))       0.84 (0.37)       0.84 (0.36)     0.015\n  white (mean (SD))       0.10 (0.30)       0.10 (0.30)     0.018\n  age (mean (SD))        25.36 (8.29)      25.82 (7.16)     0.059\n  educ (mean (SD))       10.45 (1.96)      10.35 (2.01)     0.054\n  married (mean (SD))     0.19 (0.39)       0.19 (0.39)    &lt;0.001\n  nodegree (mean (SD))    0.71 (0.46)       0.71 (0.46)    &lt;0.001\n  re74 (mean (SD))     2159.92 (4240.18) 2095.57 (4886.62)  0.014\n  re75 (mean (SD))     1119.08 (2442.29) 1532.06 (3219.25)  0.145\n\n\nGreedy pairwise matching yields, as expected, a reduced data set with 185 subjects in each group. This time all but the variable \\(re75\\) have corresponding \\(SMD&lt;0.1\\). This is not entirely satisfactory and I will attempt to balance the data set using propensity scores next"
  },
  {
    "objectID": "posts/causailty_in_observation_data/index.html#propensity-score-matching",
    "href": "posts/causailty_in_observation_data/index.html#propensity-score-matching",
    "title": "Evaluating causality in observational data",
    "section": "Propensity score matching",
    "text": "Propensity score matching\nA propensity score denoted here \\(\\pi\\) is defined as the probability that a subject \\(i\\) received treatment conditioned on the covariates \\(X\\). I.e. \\(\\pi_i = P(T=1|X_i)\\). A \\(\\pi_i=0.4\\) means there’s a 40% chance the corresponding subject will receive treatment. Covariates can increase or decrease the probability of receiving treatment. For example, if \\(X\\), simplistically the only covariate, is a boolean variable for smoking and smokers are more likely to get a particular treatment then \\(P(T=1|X=1) \\gt P(T=1|X=0)\\).\nInterestingly, two subjects may have the same propensity score in spite of having different covariate values \\(X\\), meaning they are equally likely to receive treatment. Thus reducing the data to a subset of subjects with the same \\(\\pi\\) should balance the treated and control groups. In doing so a crucial assumption in causality, ignorability i.e. how a subject ended in one or the other group can be safely ignored. In a randomized trial, the propensity score is known. In an observational study \\(\\pi\\) is unknown. However because both \\(X\\) and \\(T\\) are collected, \\(\\pi\\) can be estimated. For this I will fit a logistic regression, where the covariates are the input and the treatment variable is the output. Using this model I can get the predicted probability of treatment for each subject; i.e. the estimated \\(\\pi\\).\n\n\nCode\npsmodel &lt;- glm(treat~hispan+white+black+age+educ+married+\n                 nodegree+re74+re75,family=binomial(), data=mydata)\n\n\nThe model fit is summarized below.\n\n\nCode\n# show fit summary\nsummary(psmodel)\n\n\n\nCall:\nglm(formula = treat ~ hispan + white + black + age + educ + married + \n    nodegree + re74 + re75, family = binomial(), data = mydata)\n\nCoefficients: (1 not defined because of singularities)\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.663e+00  9.709e-01  -1.713  0.08668 .  \nhispan      -2.082e+00  3.672e-01  -5.669 1.44e-08 ***\nwhite       -3.065e+00  2.865e-01 -10.699  &lt; 2e-16 ***\nblack               NA         NA      NA       NA    \nage          1.578e-02  1.358e-02   1.162  0.24521    \neduc         1.613e-01  6.513e-02   2.477  0.01325 *  \nmarried     -8.321e-01  2.903e-01  -2.866  0.00415 ** \nnodegree     7.073e-01  3.377e-01   2.095  0.03620 *  \nre74        -7.178e-05  2.875e-05  -2.497  0.01253 *  \nre75         5.345e-05  4.635e-05   1.153  0.24884    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 751.49  on 613  degrees of freedom\nResidual deviance: 487.84  on 605  degrees of freedom\nAIC: 505.84\n\nNumber of Fisher Scoring iterations: 5\n\n\nNext I extract the propensity scores from the model object.\n\n\nCode\n# create propensity score\npscore&lt;-psmodel$fitted.values\n\n\nFinally, I use the MatchIt package to match subjects based on their propensity scores. Note that I set a seed for reproducibility, since matching randomizes data as a first step.\n\n\nCode\nset.seed(42)\nm.out&lt;-matchit(treat~hispan+white+black+age+educ+married+\n                 nodegree+re74+re75, data=mydata, method=\"nearest\")\n\n\nThe matching results are summarized below.\n\n\nCode\n# summarize the matching outcome\nsummary(m.out)\n\n\n\nCall:\nmatchit(formula = treat ~ hispan + white + black + age + educ + \n    married + nodegree + re74 + re75, data = mydata, method = \"nearest\")\n\nSummary of Balance for All Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance        0.5774        0.1822          1.7941     0.9211    0.3774\nhispan          0.0595        0.1422         -0.3498          .    0.0827\nwhite           0.0973        0.6550         -1.8819          .    0.5577\nblack           0.8432        0.2028          1.7615          .    0.6404\nage            25.8162       28.0303         -0.3094     0.4400    0.0813\neduc           10.3459       10.2354          0.0550     0.4959    0.0347\nmarried         0.1892        0.5128         -0.8263          .    0.3236\nnodegree        0.7081        0.5967          0.2450          .    0.1114\nre74         2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75         1532.0553     2466.4844         -0.2903     0.9563    0.1342\n         eCDF Max\ndistance   0.6444\nhispan     0.0827\nwhite      0.5577\nblack      0.6404\nage        0.1577\neduc       0.1114\nmarried    0.3236\nnodegree   0.1114\nre74       0.4470\nre75       0.2876\n\nSummary of Balance for Matched Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance        0.5774        0.3629          0.9739     0.7566    0.1321\nhispan          0.0595        0.2162         -0.6629          .    0.1568\nwhite           0.0973        0.3135         -0.7296          .    0.2162\nblack           0.8432        0.4703          1.0259          .    0.3730\nage            25.8162       25.3027          0.0718     0.4568    0.0847\neduc           10.3459       10.6054         -0.1290     0.5721    0.0239\nmarried         0.1892        0.2108         -0.0552          .    0.0216\nnodegree        0.7081        0.6378          0.1546          .    0.0703\nre74         2095.5737     2342.1076         -0.0505     1.3289    0.0469\nre75         1532.0553     1614.7451         -0.0257     1.4956    0.0452\n         eCDF Max Std. Pair Dist.\ndistance   0.4216          0.9740\nhispan     0.1568          1.0743\nwhite      0.2162          0.8390\nblack      0.3730          1.0259\nage        0.2541          1.3938\neduc       0.0757          1.2474\nmarried    0.0216          0.8281\nnodegree   0.0703          1.0106\nre74       0.2757          0.7965\nre75       0.2054          0.7381\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       185     185\nUnmatched     244       0\nDiscarded       0       0\n\n\nThe above is more intuitively approached with some plotting as shown below.\n\n\nCode\n# propensity score plots\nplot(m.out, type=\"hist\")\n\n\n\n\n\nWhat I am looking for with the plot above is an improvement in the overlap between the distribution of propensity scores of matched control and treated groups, relative to the raw. There is obviously some improvement and I’ll next check more concretely below how good the match is using SMD.\n\n\nCode\n# --&gt; MATCHHING WITH & WITHOUT A CALIPER\n# Matching without a caliper\n# -&gt; do greedy matching on logit (PS)\nset.seed(42)\npsmatch &lt;- Match(Tr=mydata$treat, M=1, X=pscore, replace=FALSE)\nps_matched &lt;- mydata[unlist(psmatch[c(\"index.treated\", \"index.control\")]), ]\nmatchedtab1_pscore &lt;- CreateTableOne(vars=xvars, strata=\"treat\", data=ps_matched,\n                              test=FALSE)\nprint(matchedtab1_pscore, smd=TRUE)\n\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        185               185                 \n  hispan (mean (SD))      0.21 (0.41)       0.06 (0.24)     0.453\n  black (mean (SD))       0.47 (0.50)       0.84 (0.36)     0.852\n  white (mean (SD))       0.32 (0.47)       0.10 (0.30)     0.566\n  age (mean (SD))        25.34 (10.53)     25.82 (7.16)     0.053\n  educ (mean (SD))       10.59 (2.63)      10.35 (2.01)     0.106\n  married (mean (SD))     0.21 (0.41)       0.19 (0.39)     0.041\n  nodegree (mean (SD))    0.64 (0.48)       0.71 (0.46)     0.139\n  re74 (mean (SD))     2455.47 (4352.86) 2095.57 (4886.62)  0.078\n  re75 (mean (SD))     1731.45 (2813.88) 1532.06 (3219.25)  0.066\n\n\nThe table above shows marked improvement relative to the raw data. However, variables like education (\\(educ\\)) and lack of high school degree (\\(nodegree\\)) are marginal, while race-related variables (\\(hispan\\), \\(black\\), \\(white\\)) are still too high to safely avoid confounding. Overall it is quite a bit worse than the first attempt with greedy matching I did earlier using the Mahalanobis distance; though \\(re75\\) is markedly better here.\n\nMatching with a caliper\nOne way to try to improve on the matching is to use a caliper; i.e. a threshold (maximum) distance beyond which matching is not allowed. In practice, though somewhat arbitrarily, (1) the propensity scores are logit-transformed, (2) the standard deviation (SD) is calculated, and (3) the caliper is set to 0.2 times the SD, finally (4) the matching is performed subject to the caliper. A smaller caliper, which results in fewer but better pairs, trades more variance for less bias. The code below performs the aforementioned steps.\n\n\nCode\nset.seed(42)\nlogit_pscore = qlogis(pscore)\npsmatch_calip &lt;-Match(Tr=mydata$treat, M=1, X=logit_pscore, replace=FALSE,\n                caliper=.2)\n# Note that the caliper is in St.Dev. units\nmatched_calip &lt;- mydata[unlist(psmatch_calip[c(\"index.treated\", \"index.control\")]), ]\nmatchedtab1_calip &lt;- CreateTableOne(vars=xvars, strata=\"treat\", data=matched_calip,\n                              test=FALSE)\nprint(matchedtab1_calip, smd=TRUE)\n\n\n                      Stratified by treat\n                       0                 1                 SMD   \n  n                        114               114                 \n  hispan (mean (SD))      0.11 (0.32)       0.10 (0.30)     0.057\n  black (mean (SD))       0.73 (0.45)       0.75 (0.44)     0.040\n  white (mean (SD))       0.16 (0.37)       0.16 (0.37)    &lt;0.001\n  age (mean (SD))        26.35 (10.83)     25.82 (6.93)     0.059\n  educ (mean (SD))       10.53 (2.63)      10.30 (2.29)     0.092\n  married (mean (SD))     0.26 (0.44)       0.24 (0.43)     0.061\n  nodegree (mean (SD))    0.61 (0.49)       0.64 (0.48)     0.054\n  re74 (mean (SD))     2858.97 (4816.95) 2168.29 (5590.28)  0.132\n  re75 (mean (SD))     1969.43 (3027.44) 1053.23 (2597.71)  0.325\n\n\nCode\n# NOTE the smaller number of subjects for each treatment categories, resultng\n# from dropping previously matched subjects.\n\n\nUsing the caliper has reduced the number of matched pairs down to 114. The high SMDs seen previously (without the caliper). However, for 1974 and 1975 real incomes \\(SMD &gt; 0.1\\). Thus I cannot be certain that the treatment is the only cause of the outcome. The table above suggests subjects’ earning history is still a causal factor. With this in mind, I next run an outcome analysis for all the approaches described previously. I do this at the end rather than after each outcome as a habit to prevent p-hacking.\n\n\nOutcome analyses:\nTo analyze whether the difference in outcome between the treatment and the control groups are different, I run a paired t-test on the various matched data. But first a quick function to avoid some repetition.\n\n\nCode\n# function that accepts a matched data table and runs the paired t-test.\nrun_matched_ttest&lt;- function(matched_table){\n  # get outcome data for both groups\n  treated_outcome &lt;- matched_table$outcome[matched_table$treat==1]\n  control_outcome &lt;- matched_table$outcome[matched_table$treat==0]\n  \n  # compute pairwise difference between both groups\n  diff_outcome &lt;- treated_outcome - control_outcome\n  \n  # paired t-test\n  t.test(diff_outcome)\n}\n\n\n\\(\\rightarrow\\) Greedy Mahalanobis distance matching:\n\n\nCode\nrun_matched_ttest(greedymatched)\n\n\n\n    One Sample t-test\n\ndata:  diff_outcome\nt = 0.5611, df = 184, p-value = 0.5754\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -969.9111 1740.8520\nsample estimates:\nmean of x \n 385.4705 \n\n\n\\(\\rightarrow\\) Propensity score matching (no caliper)\n\n\nCode\nrun_matched_ttest(ps_matched)\n\n\n\n    One Sample t-test\n\ndata:  diff_outcome\nt = 0.94827, df = 184, p-value = 0.3442\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -708.6934 2020.4023\nsample estimates:\nmean of x \n 655.8544 \n\n\n\\(\\rightarrow\\) Propensity score matching with caliper\n\n\nCode\nrun_matched_ttest(matched_calip)\n\n\n\n    One Sample t-test\n\ndata:  diff_outcome\nt = 1.2773, df = 113, p-value = 0.2041\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -607.6641 2812.9263\nsample estimates:\nmean of x \n 1102.631 \n\n\nThus, from greedy Mahalanobis distance matching to propensity score matching with caliper, the statistical significance of the difference between groups does increase, with propensity score matching with caliper resulting the in the smallest p-value (0.20). This is still conventionally quite high, so that it is impossible to reject the null hypothesis, i.e. that there are no difference between the groups. Moreover, there is still a potential confounding problem in all cases. E.g. the SMD for real income in `74 and `75 remains above 0.1 suggesting I was not able to remove the confounding effect. Note though that I worked for a subset of the original data. Thus I would next re-run the analysis on the entire data set, which could lead to better matching. But that is a story for another day.\nPeaceful coding!\n\n\n\n\nimage source"
  },
  {
    "objectID": "posts/crouching_desk_hidden_benefits/index.html",
    "href": "posts/crouching_desk_hidden_benefits/index.html",
    "title": "Why I went from sitting to standing to crouching desk solution.",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Some of my work…",
    "section": "",
    "text": "Why I went from sitting to standing to crouching desk solution.\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2023\n\n\nErdem Karaköylü\n\n\n\n\n\n\n  \n\n\n\n\nEvaluating causality in observational data\n\n\n\n\n\n\n\ncausality\n\n\npropensity score\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2022\n\n\nErdem Karaköylü\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my Blog-o-folio!",
    "section": "",
    "text": "This site is meant to help those - including me! - that need a different take on a technical topic of interest. The site is also meant to educate and entertain with entries Statistics, Data Science, Modeling in a variety of fields. If you dig deep enough, you’ll even fine some entries on Marine Ecology Modeling; for such was my journey. Enjoy, and by all means do leave a comment. Regardless of its polarity I find feedback a useful improvement tool.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWhy I went from sitting to standing to crouching desk solution.\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2023\n\n\nErdem Karaköylü\n\n\n\n\n\n\n  \n\n\n\n\nEvaluating causality in observational data\n\n\n\n\n\n\n\ncausality\n\n\npropensity score\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2022\n\n\nErdem Karaköylü\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Erdem Karaköylü",
    "section": "",
    "text": "I am currently a Data Scientist at Research Innovations Inc (RII), where my various projects span a number of topics including sentiment analysis, directed sentiment analysis, fake news detection, uncertainty estimation in deep learning, graph analytics, and graph neural networks. For the past 10 years, my goto programming language for these and other analytics and modeling tasks has been Python, though I sometimes fire up R studio to code up some quick statistical analysis in R.\nBefore RII I was a scientific programmer at the NASA Ocean Biology Processing Group, where I cut my teeth coding up Bayesian models to estimate the prediction uncertainty for a variety of satellite processing algorithms. These algorithms are the backbone of product generation for data generated by legacy and active satellite missions, as well as the PACE (Plankton Aerosol Cloud Ecosystem) mission scheduled to begin in 2024.\nPrior to my analytics/data science career, I trained as an oceanographer. I graduated with a PhD from Scripps Institution of Oceanography, where I primarily focused on the ecology of tiny organisms that nevertheless are tremendously important in ocean ecological processes and have ramifications for both our supply of seafood (protein!) as well as climate change."
  }
]